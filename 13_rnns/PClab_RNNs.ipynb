{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrQv9yrEbWo3"
      },
      "source": [
        "# RNNs\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Recurrent networks strictly operate on 1-D sequences. They can be used for a variety of tasks, pictured below:\n",
        "\n",
        "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width = 500>\n",
        "\n",
        "Examples of the settings in the picture:\n",
        "- one to one: vanilla MLPs that map a fixed size 1-D vector to a 1-D vector for classification or regression\n",
        "- one to many: Image captioning, given an input embedding (obtained with a CNN), a textual caption of variable length is generated.\n",
        "- many to one: (1) Sentence classification such as sentiment analysis or (2) image generation from text: in both cases variable input texts are given as input and a fixed dimensional output is generated.\n",
        "- many to many: (1) machine translation of a variable-length sentence to another variable-length sentence or (2) transcription of a variable-length .mp3 audio to a variable length text.\n",
        "- many to many (1to1 correspondence): (1) Video classification: one label for a variable number of frames in the video (the video frame embedding can be obtained with a CNN and then input into a RNN), (2) autoregressive language modeling: trying to predict the next word in the sentence, for generative purposes or (3) word classification: classify every word as belonging to a category.\n",
        "\n",
        "Note that these settings are not exclusive to recurrent neural networks. In fact, any network type that works on variable input sequences can be used towards these ends. Most famously of which are of course, Transformers, which have all but replaced RNNs in NLP and many other fields. An explanation and implementation of transformers is out of the scope of this course. It suffices to know that RNNs process input sequence sequentially through memory cells, whereas transformers do it in parallel through an $n \\times n$ attention matrix. Other than RNNs and Transformers, convolutional networks can also be used on variable length inputs: a 1D kernel can equally well convolve over a sequence of length $100$ as $1000$. It is only because of the linear layers at the end for classification requiring a specific number of input nodes that typical CNNs become applicable on only one specific input size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFhcwjLBbWo9"
      },
      "source": [
        "## Partim 1: Autoregressive modeling\n",
        "\n",
        "Autoregressive modeling is the task of trying to predict the next token in a sequence given which tokens came before it: $P(x_i | x_{i-1}, x_{i-2}, ..., x_1)$.\n",
        "\n",
        "In this PC-lab we will explore autoregressive modeling on a dataset close to home: a dataset published by the city of Ghent on how many cyclists pass by the bicycle counter at the Coupure.\n",
        "\n",
        "<img src=\"https://images0.persgroep.net/rcs/4cQwm-ofvb3eyIKMWnNf5axxLHg/diocontent/217261403/_fitwidth/694/?appId=21791a8992982cd8da851550a453bd7f&quality=0.8\" width = 500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ArqjiofwdVJx"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/gdewael/Work/MLLS/13_rnns/PClab_RNNs.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gdewael/Work/MLLS/13_rnns/PClab_RNNs.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gdewael/Work/MLLS/13_rnns/PClab_RNNs.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gdewael/Work/MLLS/13_rnns/PClab_RNNs.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gdewael/Work/MLLS/13_rnns/PClab_RNNs.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gdewael/Work/MLLS/13_rnns/PClab_RNNs.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/gdewael/teaching/main/predmod/RNNs/fietstelpaal-coupure-links-2022-gent.csv\", \"./data.csv\")\n",
        "\n",
        "data_raw = pd.read_csv(\"data.csv\", sep = \";\")\n",
        "\n",
        "data_raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kux7BEuBemUS"
      },
      "source": [
        "For the PC lab, we will aggregate the individual 5-minute observations to hour-level observations. The following code processes the data:\n",
        "\n",
        "Think for yourself what the advantages and disadvantages are of working at this reduced temporal resolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRZFEx2Od2vd"
      },
      "outputs": [],
      "source": [
        "data_raw[\"Date_hour\"] = data_raw[\"Ordening\"].str.split(':', expand = True)[0]\n",
        "data_grouped = data_raw.groupby(\"Date_hour\").sum()\n",
        "data = pd.concat([data_grouped.index.to_series().str.split('T', expand = True), data_grouped[\"Totaal\"]], axis = 1).reset_index(drop = True)\n",
        "data.columns = ['Date', 'Hour', 'n']\n",
        "data = pd.concat([data, pd.DataFrame([[\"2022-03-27\", \"02\", 0.0]], columns = data.columns)], axis = 0).sort_values([\"Date\", \"Hour\"]) # ignore this line, this is because there is no observation for this date/hour\n",
        "data[\"n_norm\"] = data[\"n\"] / data[\"n\"].max() # we min-max normalize here for learning stability\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45I4eXgtdUrm"
      },
      "source": [
        "Architecturally, autoregressive modeling of a timeseries looks like this:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/predmod/RNNs/AR.png\" width = 500>\n",
        "\n",
        "For every point in the timeseries, the *input token* consists of the value at that timepoint and, optionally, extra covariates pertaining to that timepoint. Conceptually, this is similar to the channels in a CNN, as there we also had input tokens (e.g. pixels) with multiple values (e.g. the 3 RGB values) per token. In our case, extra covariates could be for example the hour at which that timepoint was taken. This input sequence will go into the RNN, which will keep a hidden layer which acts as a memory bank. The memory bank of every input will consist of a combination of the information at that time point and the information coming in from the memory cell at the previous time point. The specific way this information is brought together depends on the specific construction of the RNN. We refer you to the theory lectures for details. The most popular constructions are the LSTM and the GRU memory cells. For every timestep, the model outputs a vector which (for this purpose) needs to be linearly recombined to one number as our goal is to predict the value of the next timestep (i.e. a regression task).\n",
        "\n",
        "Code-wise, it is important to know that for a given sequence, we have an input $x$ consisting of the timepoints in that sequence, and an output $y$, consisting of the same points, but **shifted one time-step to the left**. **Because of the directionality of the RNN, for every time-step, it will predict the next point given only the preceding ones.**\n",
        "\n",
        "In this PC-lab, we will use the hour of the timepoint as covariate.\n",
        "\n",
        "For training, we can't put all days in to our model as one sample. Just like the reason for doing batches in other networks is that: it is more computationally efficient, and it allows us to have training steps on different parts of data with some stochasticity to it, allowing us to jump out of local minima.\n",
        "\n",
        "**For RNN, another reason is that our \"actual\" neural network depth is essentially decided by our input length**, so if we send in a sample containing a thousand input tokens, we also backpropagate through a thousand layers, and our computers will surely crash. In addition, it is not reasonable to assume the number of cyclists hundreds of days past still influences the number of cyclists now. So, the problem of batching our sequence becomes one of weighing two factors: how long of a sequence can our model handle, and how much context (in number of preceding tokens) do our models need for prediction?\n",
        "\n",
        "Here we will take a batch size of 48 as a default, meaning that our samples will always coincide with two consecutive days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_xO94-nfEUJ"
      },
      "outputs": [],
      "source": [
        "def generate_batches(sequence, seqlen = 48):\n",
        "    batches = []\n",
        "    for i in np.arange(0, len(sequence) - seqlen, seqlen):\n",
        "        batches.append(sequence[i:i+seqlen])\n",
        "    return torch.stack(batches)\n",
        "\n",
        "batches = generate_batches(torch.tensor(data[[\"Hour\", \"n_norm\"]].values.astype(np.float32)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DeB-YAajJye"
      },
      "source": [
        "Let's see how a sample looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV9G5RHzmiW1"
      },
      "outputs": [],
      "source": [
        "batches.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqNij14zjIlf"
      },
      "outputs": [],
      "source": [
        "batches[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fogv0s--jiXD"
      },
      "source": [
        "We could give our input to the model like this, as the hour is a numerical value which can be interpreted using linear layers. It would make more sense to treat the hour variable as categorical and encode it using dummy variables (one-hot encoding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XAhDa7pmdNd"
      },
      "outputs": [],
      "source": [
        "one_hot_hour = torch.nn.functional.one_hot(batches[:, :, 0].long())\n",
        "\n",
        "one_hot_hour.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RTNwK3Amv21"
      },
      "outputs": [],
      "source": [
        "batches = torch.cat([batches[:, :, [1]], one_hot_hour], axis = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5h43ZQWKm3G4"
      },
      "outputs": [],
      "source": [
        "batches.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw3OsNCom4Ic"
      },
      "outputs": [],
      "source": [
        "batches[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "autQkT0vn_L5"
      },
      "source": [
        "### RNNs in PyTorch\n",
        "\n",
        "In this PC-lab we will use the GRU, but note that other RNN-types work similarly in PyTorch.\n",
        "\n",
        "[Documentation for the GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
        "\n",
        "Extra note: The weird shape expectations (such as not expecting batches to come first by default) are a consequence of optimizations that PyTorch has implemented so the RNNs run efficiently on data with variable input sequence lengths (such as sentences). For this PC lab, we have batched our sequences so that they have constant sequence length, so we can add the argument `batch_first = True`.\n",
        "\n",
        "Let's create a GRU and some toy data to see how it all works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_Z39C9soQXX"
      },
      "outputs": [],
      "source": [
        "gru = nn.GRU(input_size = 64, hidden_size = 512, batch_first = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XU72dghoRnu"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(2, 50, 64)\n",
        "output, h_n = gru(x)\n",
        "output.shape, h_n.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7h1DZBHoX9o"
      },
      "source": [
        "Explanation of the outputs: `h_n` is the hidden representation of the last hidden memory cell. It can be seen as a summarized representation of the content of the whole input (if one wants a single output for a whole sequence as in e.g. sentence classification). `output` will return the output representation of the RNN for every input token. (Look back at the picture in the introduction of this part of the PC lab (Section 2.1) for more intuition as to when to use what outputs of the RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jPd_xQpolKr"
      },
      "source": [
        "For autoregressive modeling of time series, we should have an output for every input, mainly: the prediction of the next timepoint. For our purpose, we are interested to predict a single output per timepoint, whereas we can see that this is not the case for our GRU model as it is now. We can remedy this with a simple linear layer \n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b> \n",
        "<p> Implement an autoregressive GRU for cycler forecasting by completing the code below. The model should contain a recurrent layer, and a layer that takes the outputs of the GRU at each timestep and manipulates their dimensions so that the output dimensionality of each token is equal to one. Keep in mind how many input variables we have in our dataset. (Look at the $batches$ variable).\n",
        "</p>\n",
        "\n",
        "</div>\n",
        "\n",
        "Test your model with some toy data down below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13mJg1lqomei"
      },
      "outputs": [],
      "source": [
        "class CyclerForecaster(nn.Module):\n",
        "    # def __init__() ....\n",
        "    \n",
        "    # def forward() ...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPgN3P4NpvJL"
      },
      "outputs": [],
      "source": [
        "model = CyclerForecaster()\n",
        "\n",
        "x = torch.randn(2, 48, 25)\n",
        "\n",
        "y = model(x)\n",
        "\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwqJ1fTpnCWa"
      },
      "source": [
        "To create an input and an output, we have to do the above-mentioned shifting. In practice, this means that we take all but the last timepoints to create X, and all but the first timepoints to create Y, hence creating shifted X,y pairs.\n",
        "\n",
        "Additionally, for y, we only want to keep the first variable, meaning the number of cyclists itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMw9_32bf9hj"
      },
      "outputs": [],
      "source": [
        "X = batches[:, :-1]\n",
        "\n",
        "y = batches[:, 1:]\n",
        "y = y[:, :, [0]]\n",
        "\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shw0wQUNrLiz"
      },
      "source": [
        "Data splitting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvr5sBhDhR5G"
      },
      "outputs": [],
      "source": [
        "X_train = X[:int(batches.shape[0] * 0.80)]\n",
        "X_test = X[int(batches.shape[0] * 0.80):]\n",
        "\n",
        "y_train = y[:int(batches.shape[0] * 0.80)]\n",
        "y_test = y[int(batches.shape[0] * 0.80):]\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 8, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM5AD2u7rNA2"
      },
      "source": [
        "Note that we are just taking the first 80% of the data as training and the last 20% as validation set. Normally, we shuffle our data so that we are not biased. In this case, however, we can make a case for doing it our way: because the samples are ordered by day and month, the last samples will be from the summer months, where we may expect different patterns (June: exams, July: vacations)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmpFl85TvvF0"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b> \n",
        "<p> Implement the training loop for the Cycler Forecaster using the same principles from last PC labs. Keep in mind that unlike previous PC labs, we are now training a regression task. You will also need to up the number of epochs as our dataset is quite small and each epoch only constitutes a small number of training steps.\n",
        "</p>\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RclHsVTChfYQ"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XntUGcU0wR8m"
      },
      "source": [
        "To evaluate our model beyond looking at a loss function going down: we can look at the autoregressive results for a random test sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYeAfeVhhnCA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.arange(len(y_test[0])), y_test[0])\n",
        "plt.plot(np.arange(len(y_test[0])), model(X_test[[0]]).detach().squeeze(0).numpy())\n",
        "plt.legend([\"True\", \"Predicted\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXMxdryOwuY5"
      },
      "source": [
        "You should see that at the beginning of the second day, the model overshoots its prediction by a wide margin. This is because most probably the next day is a weekend day. After that, for the prediction of the next step, we give it the true input of that (previously-wrongly predicted) timestep, and because it recognizes that by this low value it should be a weekend day, the subsequent predictions are also low.\n",
        "\n",
        "This is not really forecasting though: at every timestep, we are predicting only one timestep (hour) in advance. To really do forecasting for a longer time limit, we should feed the predictions of the model back into the model, like this:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/predmod/RNNs/generation.png\" width = 500>\n",
        "\n",
        "To perform this, we will create a helper function that extracts the next step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiyoKVzAx_sc"
      },
      "outputs": [],
      "source": [
        "def generate_next_timestep(previous, model):\n",
        "    with torch.no_grad():\n",
        "        output = model(previous.unsqueeze(0))[0, -1]\n",
        "    \n",
        "    return output\n",
        "\n",
        "def generate_n_timesteps(previous, model, n = 5):\n",
        "    for _ in range(n):\n",
        "        prediction = generate_next_timestep(previous, model)\n",
        "        # to make a next prediction, we not only need to add the prediction but also the covariates (hour) to our next input:\n",
        "        next_timestep_hour = nn.functional.one_hot((previous[-1, 1:].argmax() + 1) % 24, num_classes = 24) \n",
        "        new_input = torch.cat([prediction, next_timestep_hour])\n",
        "        previous = torch.cat([previous, new_input.unsqueeze(0)])\n",
        "\n",
        "    return previous[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROyU81wA8Ixe"
      },
      "outputs": [],
      "source": [
        "sample_index = 0 # any index in the test set\n",
        "priming_points = ... # how many observed timepoints to give the model to start predicting from\n",
        "generate_steps = ... # how many timepoints in the future the model should forecast\n",
        "\n",
        "\n",
        "plt.plot(np.arange(len(X_test[sample_index])), X_test[sample_index, :, 0])\n",
        "predictions = generate_n_timesteps(X_test[sample_index, :priming_points], model, n = generate_steps).numpy()\n",
        "plt.plot(np.arange(len(predictions)), predictions)\n",
        "plt.axvline(x = priming_points-1, color = \"green\", linestyle = \":\")\n",
        "plt.legend([\"True\", \"Predicted\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZytOD_XA0tx"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>THOUGHT EXERCISE:</b> \n",
        "<p> Play around with the code for forecasting bicyclists by changing up which sample you forecast (\"sample_index\"), how much initial input points you give the model (\"priming points\") and how far in time it should generate after that (\"generate steps\"). Can you find obvious failure states? Can you think of ways how to remedy these failure modes?\n",
        "</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLOnrUwuPoOA"
      },
      "source": [
        "Written answer:\n",
        "If the priming point is a friday and the next day a weekend day, it will still predict high numbers for the weekend. Additionally, when generating a lot of days, after a while the same pattern is predicted for every day.\n",
        "\n",
        "**Solution**: The model doesn't know what day of the week it is, resulting in a behavior in which the best the model can do is predict \"the average day\". One could encode more features for every timepoint in addition to just which hour it was, such as which day of the week it is. In addition, one could add weather and temperature variables to the model, so that the model can account for seasonal and weather impacts on the prediction. One could even go as far as adding an indicator variable that indicates whether a day is a Belgian holiday ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2N3F1zyBpQg"
      },
      "source": [
        "## Partim 2: time series classification\n",
        "\n",
        "In the next part, we will explore a many-to-one scenario. To keep things simple, we will work on the same dataset and try to predict from the series of a single day whether that day was a weekend or a weekday.\n",
        "\n",
        "To do this, we first need to know which days were weekdays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itJrohODCD5a"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7msK_1LRVLM"
      },
      "source": [
        "Following code adds an indicator variable to the data that signifies if the date is a weekday or not. We'll use this as \"y\" values to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3ZcSVjICFRo"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "def isweekday(date):\n",
        "    return datetime.fromisoformat(date).weekday() < 5\n",
        "\n",
        "data[\"isweekday\"] = [int(isweekday(i)) for i in data[\"Date\"]]\n",
        "data \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQKStrAtHLVs"
      },
      "source": [
        "With this data added, we can process our data into X and y:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAhtjoCMDdlM"
      },
      "outputs": [],
      "source": [
        "batches = generate_batches(torch.tensor(data[[\"isweekday\", \"Hour\", \"n_norm\"]].values.astype(np.float32)), seqlen = 24)\n",
        "y = batches[:, 0, 0]\n",
        "X = batches[:, :, 1:]\n",
        "\n",
        "one_hot_hour = torch.nn.functional.one_hot(X[:, :, 0].long())\n",
        "X = torch.cat([X[:, :, [1]], one_hot_hour], axis = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvtDFbMhDh4U"
      },
      "outputs": [],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DBAGqj4ReEj"
      },
      "source": [
        "Examine if the shapes are conforming to your expectations as to what we expect it to be for timeseries classification. What does every dimension in X and y signify?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DMgGIEGGc29"
      },
      "outputs": [],
      "source": [
        "X_train = X[:int(batches.shape[0] * 0.80)]\n",
        "X_test = X[int(batches.shape[0] * 0.80):]\n",
        "\n",
        "y_train = y[:int(batches.shape[0] * 0.80)]\n",
        "y_test = y[int(batches.shape[0] * 0.80):]\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 8, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L15DJ0eeHHpL"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b> \n",
        "<p> Implement a time series classification model by copying the code above and adapting it.\n",
        "\n",
        "Steps you need to take:\n",
        "- Change the RNN model so that it is adapted for many to one tasks\n",
        "- Change the loss function for binary classification\n",
        "- Implement a way to keep track of accuracies instead of only losses\n",
        "</p>\n",
        "\n",
        "</div>\n",
        "\n",
        "You should be able to obtain an accuracy of at least 90%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBQaLX3xHvQ6"
      },
      "outputs": [],
      "source": [
        "class TimeseriesClassifier(nn.Module):\n",
        "    # def __init__() ....\n",
        "    \n",
        "    # def forward() ...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOhRw5MBIscM"
      },
      "outputs": [],
      "source": [
        "# TRAINING CODE GOES HERE ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fauaIT83dN4X"
      },
      "source": [
        "Let's plot all the test data and see what the model still got wrong. In the following code. Green timeseries are weekdays and blue timeseries are weekends. The transparent ones are correctly predicted, whereas the dotted ones are predicted wrongly. (So a blue dotted line means a weekend predicted as being a weekday and vice-versa)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynouhvU1dNeO"
      },
      "outputs": [],
      "source": [
        "preds = (model(X_test) > 0).detach().int().numpy()\n",
        "trues = y_test.numpy()\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    plt.plot(np.arange(24), X_test[i, :, 0].numpy(),\n",
        "             color = [\"blue\", \"green\"][int(trues[i])],\n",
        "             ls = [\":\", \"-\"][int(trues[i] == preds[i])],\n",
        "             alpha = [1, 0.1][int(trues[i] == preds[i])])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('gaetan_predmod')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ef9dbff06f2d3990401ab7bd73f166712843564b695d78ee6ca1b88459deb91e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
